project_name: "neptune_train"
project_save_dir: "./neptune_outputs" # Directory to save logs and checkpoints
training: true # Set to false for testing mode
accelerator: "gpu" # Hardware accelerator (e.g., "gpu", "cpu", "tpu")
num_devices: 1 # Number of devices to use (e.g., GPUs)
dataloader: "prometheus"  # 'prometheus' or 'icecube_parquet'
checkpoint: "" # Path to a checkpoint file to load model weights from (optional)
resume_training: false # Set to true to resume training state from checkpoint

data_options:
  # List of directories containing training data files (*.parquet)
  train_data_files:
    - "/path/to/prometheus/train/data"
  # List of [start, end] index ranges for files in each train_data_files directory
  train_data_file_ranges:
    - [0, 100]
  # List of directories containing validation data files (*.parquet)
  valid_data_files:
    - "/path/to/prometheus/valid/data"
  # List of [start, end] index ranges for files in each valid_data_files directory
  valid_data_file_ranges:
    - [0, 20]
  use_latent_representation: true # Use pre-calculated latent features if available in dataset
  geo_dict_path: "/path/to/prometheus/geo_dict.npy" # Path for Prometheus geometry mapping
  geo_inverse_dict_path: "/path/to/icecube/geo_inverse_dict.npy" # Path for IceCube geometry mapping

model_options:
  in_channels: 6            # Number of input feature channels per point
  num_patches: 128          # Maximum number of tokens/patches generated by the tokenizer
  token_dim: 768            # Dimensionality of each token embedding
  num_layers: 12            # Number of layers in the Transformer Encoder
  num_heads: 12             # Number of attention heads in the Transformer Encoder
  hidden_dim: 3072          # Dimension of the feedforward network within the Transformer
  dropout: 0.1              # Dropout rate used in the model
  downstream_task: "angular_reco" # Task type: 'angular_reco' or 'energy_reco'
  loss_fn: "angular_distance"   # Loss: 'angular_distance' or 'vmf' for angular; 'log_cosh' or 'gaussian_nll' for energy
  k_neighbors: 16           # Number of nearest neighbors for tokenization
  pool_method: "max"        # Pooling method for tokenization: 'max' or 'mean'

training_options:
  batch_size: 128           # Number of samples per batch
  lr: 1e-3                  # Learning rate
  lr_schedule: [10, 2]      # CosineAnnealingWarmRestarts params [T_0, T_mult]
  weight_decay: 1e-5        # Weight decay (L2 penalty)
  epochs: 100               # Maximum number of training epochs
  save_epochs: 5            # Frequency (in epochs) for saving checkpoints
  num_workers: 8            # Number of worker processes for data loading 